{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import re\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up tqdm for progress bars\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Rows and Sample Randomly\n",
    "\n",
    "We will now drop any rows that are irrelevant to us and save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/Books_rating.csv')\n",
    "\n",
    "# Keep only relevant columns and rename for ease of access\n",
    "df = df[['review/summary', 'review/text', 'review/score']]\n",
    "df.columns = ['summary', 'text', 'score']\n",
    "\n",
    "# Sample a portion of the data\n",
    "n = 250000\n",
    "df = df.sample(n=n, random_state=1)\n",
    "\n",
    "# Save to a new CSV file and view data\n",
    "df.to_csv('data/Books_rating_relevant_columns.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords, Lemmatization, and Vectorization\n",
    "\n",
    "Process the text to remove stopwords, lemmatize, strip unneeded characters, then vectorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing: Remove stopwords, lemmatize, strip unneeded characters, then vectorize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def stemmer(text):\n",
    "    if text != text:\n",
    "        return ''\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if not w in stopwords]\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemmer to text\n",
    "df['stemmed_text'] = df['text'].progress_apply(stemmer)\n",
    "df['stemmed_summary'] = df['summary'].progress_apply(stemmer)\n",
    "df['stemmed_summary_text'] = df['stemmed_summary'] + ' ' + df['stemmed_text']\n",
    "\n",
    "# Remove trailing spaces\n",
    "df['stemmed_summary_text'] = df['stemmed_summary_text'].progress_apply(\n",
    "    lambda x: x.strip())\n",
    "\n",
    "# Clean up NaN and remove empty rows\n",
    "df.fillna('', inplace=True)\n",
    "df = df[df['stemmed_summary_text'] != '']\n",
    "\n",
    "# Drop unused columns\n",
    "df.drop(columns=['text', 'summary', 'stemmed_summary',\n",
    "        'stemmed_text'], inplace=True)\n",
    "\n",
    "# Save to a new CSV file and view the data\n",
    "df.to_csv('data/Books_rating_stemmed.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing\n",
    "\n",
    "Vectorize the text using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text using TF-IDF\n",
    "df = pd.read_csv('data/Books_rating_stemmed.csv')\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['stemmed_summary_text'], df['score'], test_size=0.2, random_state=1)\n",
    "\n",
    "# Initialize the vectorizer and fit on the training set\n",
    "vectorizer = TfidfVectorizer(max_features=30_000, sublinear_tf=True)\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# Transform training and test sets\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Get the shape of training and test sets\n",
    "print(f'Shape of training set: {X_train.shape}')\n",
    "print(f'Shape of testing set: {X_test.shape}')\n",
    "\n",
    "# Save vectorizer to disk\n",
    "pickle.dump(vectorizer, open('models/vectorizer.pkl', 'wb'))\n",
    "print('Vectorizer saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "First testing different kinds of models, then further testing with hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store model instances\n",
    "models = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'LinearSVC': LinearSVC()\n",
    "}\n",
    "\n",
    "# Create lists to store model names and accuracies for plotting\n",
    "model_names = []\n",
    "accuracies = []\n",
    "\n",
    "# Iterate through models\n",
    "for name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print classification report for more detailed metrics\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Store model name and accuracy for plotting\n",
    "    model_names.append(name)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plotting accuracies\n",
    "plt.bar(model_names, accuracies)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we decided to further test the LogisticRegression and LinearSVC since the Naive Bayes did not produce favorable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We are going to test the logistic regression model with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Logistic Regression with various hyperparameters\n",
    "penalty = 'l2'\n",
    "C = [0.01, 0.1, 1]\n",
    "max_iter = [500, 1000]\n",
    "solver = ['liblinear', 'saga']\n",
    "n_jobs = -1\n",
    "random_state = 42\n",
    "\n",
    "# Create lists to store hyperparameters and accuracies for plotting\n",
    "params = []\n",
    "accuracies = []\n",
    "\n",
    "# Iterate through hyperparameters\n",
    "for c in C:\n",
    "    for i in max_iter:\n",
    "        for s in solver:\n",
    "            # Create model instance\n",
    "            model = LogisticRegression(penalty=penalty, max_iter=i, solver=s, n_jobs=n_jobs, random_state=random_state)\n",
    "            \n",
    "            # Test model and store results\n",
    "            accuracy = model.fit(X_train, y_train).score(X_test, y_test)\n",
    "            params.append(f'C={c}, max_iter={i}, solver={s}')\n",
    "            accuracies.append(accuracy)\n",
    "                \n",
    "# Plotting accuracies\n",
    "plt.bar(params, accuracies)\n",
    "plt.xlabel('Hyperparameters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Logistic Regression Hyperparameters')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Print accuracies\n",
    "for i in range(len(params)):\n",
    "    print(f'{params[i]}: {accuracies[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 and L2 regularization refer to Lasso and Ridge regression respectively. Lasso regression penalizes the absolute size of coefficients which results in some coefficients being set to zero. Ridge regression penalizes the squared size of coefficients which results in smaller coefficients but never zero.\n",
    "\n",
    "Ridge tends to perform better for many significant predictors, while Lasso is more effective when only a few predictors are actually significant​​​. So, we decided to only test with Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC\n",
    "\n",
    "We are going to test the LinearSVC model with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Linear SVC with various hyperparameters\n",
    "penalty = 'l2'\n",
    "C = [0.01, 0.1, 1]\n",
    "max_iter = [500, 1000]\n",
    "loss = ['hinge', 'squared_hinge']\n",
    "random_state = 42\n",
    "\n",
    "# Create lists to store hyperparameters and accuracies for plotting\n",
    "params = []\n",
    "accuracies = []\n",
    "\n",
    "# Iterate through hyperparameters\n",
    "for c in C:\n",
    "    for i in max_iter:\n",
    "        for l in loss:\n",
    "            # Create model instance\n",
    "            model = LinearSVC(C=c, penalty=penalty, max_iter=i, loss=l, random_state=random_state)\n",
    "            \n",
    "            # Test model and store results\n",
    "            accuracy = model.fit(X_train, y_train).score(X_test, y_test)\n",
    "            params.append(f'C={c}, max_iter={i}, loss={l}')\n",
    "            accuracies.append(accuracy)\n",
    "                \n",
    "# Plotting accuracies\n",
    "plt.bar(params, accuracies)\n",
    "plt.xlabel('Hyperparameters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Linear SVC Hyperparameters')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Print accuracies\n",
    "for i in range(len(params)):\n",
    "    print(f'{params[i]}: {accuracies[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started to notice that certain hyperparameters do not affect the accuracy at all in the context of this problem. You'll notice repitition in the output. However, logistic regression does edge out LinearSVC by just a little bit so we decided to stick with the configuration that produced those results. Honestly, being just the saga solver.\n",
    "\n",
    "Chosen Model: `LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1, solver='saga')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementing Our Model\n",
    "\n",
    "So, as a next step (before we attempt a nueral network) we decided to use sentiment analysis via VADER scores. We will then add these scores as features to our model and see if it improves the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from scipy import sparse as sp\n",
    "\n",
    "# Download the VADER lexicon\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the VADER Sentiment Intensity Analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER sentiment analysis to the original 'text' column\n",
    "df['vader_scores'] = df['stemmed_summary_text'].progress_apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "\n",
    "# Save the DataFrame with VADER scores to a new CSV file\n",
    "df.to_csv('data/Books_rating_vader_scores.csv', index=False)\n",
    "\n",
    "# View the updated DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform additional testing using the new feature `vader_scores` and observe any changes in model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text and VADER scores using TF-IDF\n",
    "X_text = vectorizer.transform(df['stemmed_summary_text'])\n",
    "X_vader = df['vader_scores'].values.reshape(-1, 1)\n",
    "\n",
    "# Concatenate the TF-IDF and VADER score features\n",
    "X_combined = sp.hstack([X_text, X_vader], format='csr')\n",
    "\n",
    "# Create train and test sets\n",
    "X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, df['score'], test_size=0.2, random_state=1)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model_combined = LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1, solver='saga')\n",
    "\n",
    "# Fit the model on the combined features\n",
    "model_combined.fit(X_train_combined, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_combined = model_combined.predict(X_test_combined)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_combined = accuracy_score(y_test, y_pred_combined)\n",
    "print(f\"Combined Model Accuracy: {accuracy_combined}\")\n",
    "print(\"Classification Report for Combined Model:\")\n",
    "print(classification_report(y_test, y_pred_combined))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's observe and compare the results before and after adding VADER scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a comparison of the accuracies\n",
    "plt.bar(['Text Only', 'Combined'], [accuracy, accuracy_combined])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def textblob_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "df['textblob_scores'] = df['stemmed_summary_text'].progress_apply(textblob_sentiment)\n",
    "\n",
    "df['review_length'] = df['stemmed_summary_text'].apply(len)\n",
    "df['exclamation_count'] = df['stemmed_summary_text'].apply(lambda x: x.count('!'))\n",
    "\n",
    "from scipy import sparse as sp\n",
    "\n",
    "# TextBlob sentiment analysis\n",
    "df['textblob_scores'] = df['stemmed_summary_text'].apply(textblob_sentiment)\n",
    "\n",
    "# Additional features\n",
    "df['review_length'] = df['stemmed_summary_text'].apply(len)\n",
    "df['exclamation_count'] = df['stemmed_summary_text'].apply(lambda x: x.count('!'))\n",
    "\n",
    "# Combine TF-IDF, VADER, and additional features\n",
    "X_textblob = df['textblob_scores'].values.reshape(-1, 1)\n",
    "X_length = df['review_length'].values.reshape(-1, 1)\n",
    "X_exclamation = df['exclamation_count'].values.reshape(-1, 1)\n",
    "\n",
    "X_combined = sp.hstack([X_text, X_textblob, X_length, X_exclamation], format='csr')\n",
    "\n",
    "# Create train and test sets\n",
    "X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, df['score'], test_size=0.2, random_state=1)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model_combined = LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1, solver='saga')\n",
    "\n",
    "# Fit the model on the combined features\n",
    "model_combined.fit(X_train_combined, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_combined = model_combined.predict(X_test_combined)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_combined = accuracy_score(y_test, y_pred_combined)\n",
    "print(f\"Combined Model Accuracy: {accuracy_combined}\")\n",
    "print(\"Classification Report for Combined Model:\")\n",
    "print(classification_report(y_test, y_pred_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the preprocessed data\n",
    "df = pd.read_csv('data/Books_rating_vader_scores.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['stemmed_summary_text'], df['score'], test_size=0.2, random_state=1)\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=30_000, sublinear_tf=True)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Build a simple neural network\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=30_000, output_dim=128, input_length=X_train_tfidf.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5, activation='linear'))  # Use 'linear' activation for regression tasks\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Set up early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Convert sparse matrices to dense arrays for compatibility with Keras\n",
    "X_train_dense = X_train_tfidf.toarray()\n",
    "X_test_dense = X_test_tfidf.toarray()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_dense, y_train, epochs=10, batch_size=32, validation_data=(X_test_dense, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "model.save('models/nn_model.h5')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_nn = model.predict(X_test_tfidf).flatten().round().astype(int)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "print(f\"Neural Network Accuracy: {accuracy_nn}\")\n",
    "print(\"Classification Report for Neural Network:\")\n",
    "print(classification_report(y_test, y_pred_nn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
