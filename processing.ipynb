{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "## Remove Rows and Sample Randomly\n",
    "\n",
    "We will now drop any rows that are irrelevant to us and save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2896109</th>\n",
       "      <td>Best edition of this classic.</td>\n",
       "      <td>I've always recommended this Yale edition of F...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381153</th>\n",
       "      <td>Great Book!!</td>\n",
       "      <td>This is required reading for my 16 yr old son....</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028690</th>\n",
       "      <td>Not just a book for consultant</td>\n",
       "      <td>Plain-spoken, finished the book only has taken...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945977</th>\n",
       "      <td>Outrageously Bad</td>\n",
       "      <td>Wow... this is one of the most ridiculous stor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812693</th>\n",
       "      <td>Cunning and determination</td>\n",
       "      <td>A crew has mutinied and threatens to hang thei...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                summary  \\\n",
       "2896109   Best edition of this classic.   \n",
       "2381153                    Great Book!!   \n",
       "1028690  Not just a book for consultant   \n",
       "1945977                Outrageously Bad   \n",
       "2812693       Cunning and determination   \n",
       "\n",
       "                                                      text  score  \n",
       "2896109  I've always recommended this Yale edition of F...    5.0  \n",
       "2381153  This is required reading for my 16 yr old son....    5.0  \n",
       "1028690  Plain-spoken, finished the book only has taken...    4.0  \n",
       "1945977  Wow... this is one of the most ridiculous stor...    1.0  \n",
       "2812693  A crew has mutinied and threatens to hang thei...    4.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Books_rating.csv')\n",
    "\n",
    "# preserve relevant columns and rename for ease of access\n",
    "df = df[['review/summary', 'review/text', 'review/score']]\n",
    "df.columns = ['summary', 'text', 'score']\n",
    "\n",
    "# sample a portion of the data\n",
    "n = 500_000\n",
    "df = df.sample(n=n, random_state=1)\n",
    "\n",
    "# save to new csv file and view data\n",
    "df.to_csv('data/Books_rating_relevant_columns.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords, Lemmatization, and Vectorization\n",
    "\n",
    "Process the text to remove stopwords, lemmatize, strip unneeded characters, then vectorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def stemmer(text):\n",
    "    if text != text:\n",
    "        return ''\n",
    "\n",
    "    # clean the text\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # tokenize and remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if not w in stopwords]\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [03:09<00:00, 2642.66it/s]\n",
      "100%|██████████| 500000/500000 [00:17<00:00, 29370.41it/s]\n",
      "100%|██████████| 500000/500000 [00:00<00:00, 2473374.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>stemmed_summary_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>best edition classic always recommended yale e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>great book required reading 16 yr old son book...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>book consultant plain spoken finished book tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>outrageously bad wow one ridiculous story ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>cunning determination crew mutinied threatens ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                               stemmed_summary_text\n",
       "0    5.0  best edition classic always recommended yale e...\n",
       "1    5.0  great book required reading 16 yr old son book...\n",
       "2    4.0  book consultant plain spoken finished book tak...\n",
       "3    1.0  outrageously bad wow one ridiculous story ever...\n",
       "4    4.0  cunning determination crew mutinied threatens ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/Books_rating_relevant_columns.csv')\n",
    "\n",
    "# apply stemmer to text\n",
    "df['stemmed_text'] = df['text'].progress_apply(stemmer)\n",
    "df['stemmed_summary'] = df['summary'].progress_apply(stemmer)\n",
    "df['stemmed_summary_text'] = df['stemmed_summary'] + ' ' + df['stemmed_text']\n",
    "\n",
    "# remove trailing spaces\n",
    "df['stemmed_summary_text'] = df['stemmed_summary_text'].progress_apply(\n",
    "    lambda x: x.strip())\n",
    "\n",
    "# clean up NaN and remove empty rows\n",
    "df.fillna('', inplace=True)\n",
    "df = df[df['stemmed_summary_text'] != '']\n",
    "\n",
    "# drop unused columns\n",
    "df.drop(columns=['text', 'summary', 'stemmed_summary',\n",
    "        'stemmed_text'], inplace=True)\n",
    "\n",
    "# save to new csv file and view the data\n",
    "df.to_csv('data/Books_rating_stemmed.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing\n",
    "\n",
    "Vectorize the text using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (499999, 2)\n",
      "Shape of training set: (399999, 30000)\n",
      "Shape of testing set: (100000, 30000)\n",
      "Vectorizer saved!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/Books_rating_stemmed.csv')\n",
    "print(f'Shape of dataframe: {df.shape}')\n",
    "\n",
    "# create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['stemmed_summary_text'], df['score'], test_size=0.2, random_state=1)\n",
    "\n",
    "# init the vectorizer and fit on the training set\n",
    "vectorizer = TfidfVectorizer(max_features=30_000, sublinear_tf=True)\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# transform training and test sets\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# get shape of training and test sets\n",
    "print(f'Shape of training set: {X_train.shape}')\n",
    "print(f'Shape of testing set: {X_test.shape}')\n",
    "\n",
    "# save vectorizer to disk\n",
    "pickle.dump(vectorizer, open('models/vectorizer.pkl', 'wb'))\n",
    "print('Vectorizer saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "First testing different kinds of models, then further testing with hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for MultinomialNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.22      0.34      6704\n",
      "         2.0       0.47      0.01      0.03      5135\n",
      "         3.0       0.39      0.03      0.06      8384\n",
      "         4.0       0.38      0.06      0.10     19548\n",
      "         5.0       0.63      0.99      0.77     60229\n",
      "\n",
      "    accuracy                           0.62    100000\n",
      "   macro avg       0.52      0.26      0.26    100000\n",
      "weighted avg       0.56      0.62      0.51    100000\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m     model_names\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m     39\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mmse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(mse)\n\u001b[1;32m     41\u001b[0m     rmse\u001b[38;5;241m.\u001b[39mappend(rmse)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Plotting accuracies\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store model instances\n",
    "models = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'LogisticRegression': LogisticRegression(n_jobs=-1, solver='saga'),\n",
    "    'LinearSVC': LinearSVC(),\n",
    "    # 'KNN': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),\n",
    "    'SDGClassifier': SGDClassifier(n_jobs=-1),\n",
    "    # 'RandomForestClassifier': RandomForestClassifier(n_jobs=-1),\n",
    "    # 'GradientBoostingClassifier': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Create lists to store model names and accuracies for plotting\n",
    "model_names = []\n",
    "accuracies = []\n",
    "mses = []\n",
    "rmses = []\n",
    "\n",
    "# Iterate through models\n",
    "for name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # calculate MSE and RMSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    # Print classification report for more detailed metrics\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Store model name and accuracy for plotting\n",
    "    model_names.append(name)\n",
    "    accuracies.append(accuracy)\n",
    "    mses.append(mse)\n",
    "    rmses.append(rmse)\n",
    "\n",
    "# Plotting accuracies\n",
    "plt.bar(model_names, accuracies)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracies')\n",
    "plt.show()\n",
    "\n",
    "# Plotting MSE\n",
    "plt.bar(model_names, mses)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Model MSE')\n",
    "plt.show()\n",
    "\n",
    "# Plotting RMSE\n",
    "plt.bar(model_names, rmses)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Model RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we decided to further test the LogisticRegression and LinearSVC since the Naive Bayes did not produce favorable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
