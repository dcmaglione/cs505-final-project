{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models\n",
    "\n",
    "Now that we tried non-machine learning models, let's try some machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/mfg9xrtx4qn70d9q4pb41xfr0000gn/T/ipykernel_37392/1992963669.py:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file = 'glove.6B/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>stemmed_summary_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>best edition classic always recommended yale e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>great book required reading 16 yr old son book...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>book consultant plain spoken finished book tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>outrageously bad wow one ridiculous story ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>cunning determination crew mutinied threatens ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                               stemmed_summary_text\n",
       "0    5.0  best edition classic always recommended yale e...\n",
       "1    5.0  great book required reading 16 yr old son book...\n",
       "2    4.0  book consultant plain spoken finished book tak...\n",
       "3    1.0  outrageously bad wow one ridiculous story ever...\n",
       "4    4.0  cunning determination crew mutinied threatens ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('data/Books_rating_stemmed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove model from file\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 503993/999999 [08:45<08:36, 959.63it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m stemmed_summary_text:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m glove_model:\n\u001b[0;32m---> 14\u001b[0m         review_matrix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mglove_model\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m         num_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_words \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py:395\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py:445\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors[index]\n\u001b[0;32m--> 445\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetflags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# disallow direct tampering that would invalidate `norms` etc\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is a DataFrame with columns 'rating' and 'review'\n",
    "review_term_matrix = []\n",
    "\n",
    "for score, stemmed_summary_text in tqdm(df[['score', 'stemmed_summary_text']].itertuples(index=False), total=len(df)):\n",
    "    review_matrix = np.zeros(glove_model.vector_size)\n",
    "\n",
    "    num_words = 0\n",
    "\n",
    "    for word in stemmed_summary_text:\n",
    "        if word in glove_model:\n",
    "            review_matrix += glove_model[word]\n",
    "            num_words += 1\n",
    "\n",
    "    if num_words > 0:\n",
    "        review_matrix /= num_words\n",
    "\n",
    "    review_array = [review_matrix, score]\n",
    "    review_term_matrix.append(review_array)\n",
    "\n",
    "review_term_matrix = np.array(review_term_matrix)\n",
    "\n",
    "print(review_term_matrix.shape)\n",
    "\n",
    "# save the review term matrix\n",
    "np.save('data/review_term_matrix.npy', review_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_term_matrix):\n",
    "        self.review_term_matrix = review_term_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.review_term_matrix)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.review_term_matrix[idx]\n",
    "\n",
    "review_term_matrix = np.load('data/review_term_matrix.npy', allow_pickle=True)\n",
    "dataset = ReviewDataset(review_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "def custom_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    return [torch.FloatTensor(data), torch.LongTensor(target)]\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/mfg9xrtx4qn70d9q4pb41xfr0000gn/T/ipykernel_37392/190193581.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1695391836761/work/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  return [torch.FloatTensor(data), torch.LongTensor(target)]\n",
      "/var/folders/wk/mfg9xrtx4qn70d9q4pb41xfr0000gn/T/ipykernel_37392/190193581.py:10: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return [torch.FloatTensor(data), torch.LongTensor(target)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.0463, Accuracy: 0.6024, MSE: 1.1450, RMSE: 1.0687\n",
      "Epoch: 2, Loss: 1.0554, Accuracy: 0.6024, MSE: 1.1406, RMSE: 1.0667\n",
      "Epoch: 3, Loss: 1.0951, Accuracy: 0.6024, MSE: 1.1412, RMSE: 1.0668\n",
      "Epoch: 4, Loss: 1.0715, Accuracy: 0.6024, MSE: 1.1405, RMSE: 1.0669\n",
      "Epoch: 5, Loss: 1.2419, Accuracy: 0.6024, MSE: 1.1363, RMSE: 1.0648\n",
      "Epoch: 6, Loss: 1.0351, Accuracy: 0.6024, MSE: 1.1355, RMSE: 1.0645\n",
      "Epoch: 7, Loss: 1.3200, Accuracy: 0.6024, MSE: 1.1358, RMSE: 1.0645\n",
      "Epoch: 8, Loss: 0.9218, Accuracy: 0.6024, MSE: 1.1364, RMSE: 1.0646\n",
      "Epoch: 9, Loss: 1.1079, Accuracy: 0.6023, MSE: 1.1355, RMSE: 1.0645\n",
      "Epoch: 10, Loss: 1.2421, Accuracy: 0.6024, MSE: 1.1340, RMSE: 1.0636\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network architecture\n",
    "class ReviewNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ReviewNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set the input size, hidden size, and output size based on the GloVe vector size and the number of classes (5 scores)\n",
    "input_size = glove_model.vector_size\n",
    "hidden_size = 512  # You can adjust this based on your needs\n",
    "output_size = 5  # Number of classes (scores)\n",
    "\n",
    "# Move the model and data to M1 GPU\n",
    "is_gpu = torch.backends.mps.is_available()\n",
    "\n",
    "if is_gpu:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = ReviewNet(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = torch.FloatTensor(data)  # Convert data to torch.FloatTensor\n",
    "        output = model(data)\n",
    "        target = torch.LongTensor(target) - 1  # Adjust target values to start from 0\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set, get loss, accuracy, mse, and rmse\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    mse = 0\n",
    "    rmse = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = torch.FloatTensor(data)\n",
    "            output = model(data)\n",
    "            target = torch.LongTensor(target) - 1\n",
    "            loss = criterion(output, target)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "\n",
    "            correct += (predicted == target).sum().item()\n",
    "            mse += loss.item() * target.size(0)\n",
    "            rmse += (loss.item() ** 0.5) * target.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    mse /= total\n",
    "    rmse /= total\n",
    "\n",
    "    print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}, MSE: {:.4f}, RMSE: {:.4f}'.format(epoch + 1, loss.item(), accuracy, mse, rmse))\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'review_net.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
